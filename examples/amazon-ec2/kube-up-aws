0 %> env KUBERNETES_PROVIDER=aws bash -x ./kube-up.sh 
+ set -o errexit
+ set -o nounset
+ set -o pipefail
++ dirname ./kube-up.sh
+ KUBE_ROOT=./..
+ '[' -f ./../cluster/env.sh ']'
+ source ./../cluster/kube-env.sh
++ KUBERNETES_PROVIDER=aws
++ [[ -z '' ]]
++ declare -r 'color_start=\033['
++ declare -r 'color_red=\033[0;31m'
++ declare -r 'color_yellow=\033[0;33m'
++ declare -r 'color_green=\033[0;32m'
++ declare -r 'color_norm=\033[0m'
+ source ./../cluster/kube-util.sh
+++ dirname ./../cluster/kube-util.sh
++ KUBE_ROOT=./../cluster/..
++ '[' -n aws ']'
++ PROVIDER_UTILS=./../cluster/../cluster/aws/util.sh
++ '[' -f ./../cluster/../cluster/aws/util.sh ']'
++ source ./../cluster/../cluster/aws/util.sh
++++ dirname ./../cluster/../cluster/aws/util.sh
+++ KUBE_ROOT=./../cluster/../cluster/aws/../..
+++ source ./../cluster/../cluster/aws/../../cluster/aws/config-default.sh
++++ ZONE=us-west-2a
++++ MASTER_SIZE=t2.micro
++++ MINION_SIZE=t2.micro
++++ NUM_MINIONS=4
++++ AWS_S3_REGION=us-east-1
++++ DOCKER_STORAGE=aufs
++++ INSTANCE_PREFIX=kubernetes
++++ CLUSTER_ID=kubernetes
++++ AWS_SSH_KEY=/Users/ilya/.ssh/kube_aws_rsa
++++ IAM_PROFILE_MASTER=kubernetes-master
++++ IAM_PROFILE_MINION=kubernetes-minion
++++ LOG=/dev/null
++++ MASTER_DISK_TYPE=gp2
++++ MASTER_DISK_SIZE=20
++++ MASTER_ROOT_DISK_TYPE=gp2
++++ MASTER_ROOT_DISK_SIZE=8
++++ MINION_ROOT_DISK_TYPE=gp2
++++ MINION_ROOT_DISK_SIZE=32
++++ MASTER_NAME=kubernetes-master
++++ MASTER_TAG=kubernetes-master
++++ MINION_TAG=kubernetes-minion
++++ MINION_SCOPES=
++++ POLL_SLEEP_INTERVAL=3
++++ SERVICE_CLUSTER_IP_RANGE=10.0.0.0/16
++++ CLUSTER_IP_RANGE=10.244.0.0/16
++++ MASTER_IP_RANGE=10.246.0.0/24
++++ MASTER_RESERVED_IP=
++++ ENABLE_CLUSTER_MONITORING=influxdb
++++ ENABLE_NODE_LOGGING=true
++++ LOGGING_DESTINATION=elasticsearch
++++ ENABLE_CLUSTER_LOGGING=true
++++ ELASTICSEARCH_LOGGING_REPLICAS=1
++++ [[ false == \t\r\u\e ]]
++++ ENABLE_CLUSTER_DNS=true
++++ DNS_SERVER_IP=10.0.0.10
++++ DNS_DOMAIN=cluster.local
++++ DNS_REPLICAS=1
++++ ENABLE_CLUSTER_UI=true
++++ ADMISSION_CONTROL=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
++++ ENABLE_MINION_PUBLIC_IP=true
++++ KUBE_OS_DISTRIBUTION=vivid
++++ KUBE_MINION_IMAGE=
++++ COREOS_CHANNEL=alpha
++++ CONTAINER_RUNTIME=docker
++++ RKT_VERSION=0.5.5
+++ source ./../cluster/../cluster/aws/../../cluster/common.sh
++++ set -o errexit
++++ set -o nounset
++++ set -o pipefail
+++++ dirname ./../cluster/../cluster/aws/../../cluster/common.sh
++++ KUBE_ROOT=./../cluster/../cluster/aws/../../cluster/..
++++ DEFAULT_KUBECONFIG=/Users/ilya/.kube/config
++++ KUBE_VERSION_REGEX='^v(0|[1-9][0-9]*)\.(0|[1-9][0-9]*)\.(0|[1-9][0-9]*)$'
++++ KUBE_CI_VERSION_REGEX='^v(0|[1-9][0-9]*)\.(0|[1-9][0-9]*)\.(0|[1-9][0-9]*)-(.*)$'
+++ ALLOCATE_NODE_CIDRS=true
+++ NODE_INSTANCE_PREFIX=kubernetes-minion
+++ ASG_NAME=kubernetes-minion-group
+++ MASTER_DISK_ID=
+++ [[ vivid == \u\b\u\n\t\u ]]
+++ case "${KUBE_OS_DISTRIBUTION}" in
+++ source ./../cluster/../cluster/aws/../../cluster/../cluster/aws/vivid/util.sh
++++ source ./../cluster/../cluster/aws/../../cluster/../cluster/aws/trusty/common.sh
++++ SSH_USER=ubuntu
+++ AWS_REGION=us-west-2
+++ export AWS_DEFAULT_REGION=us-west-2
+++ AWS_DEFAULT_REGION=us-west-2
+++ AWS_CMD='aws --output json ec2'
+++ AWS_ELB_CMD='aws --output json elb'
+++ AWS_ASG_CMD='aws --output json autoscaling'
+++ INTERNAL_IP_BASE=172.20.0
+++ MASTER_IP_SUFFIX=.9
+++ MASTER_INTERNAL_IP=172.20.0.9
+++ MASTER_SG_NAME=kubernetes-master-kubernetes
+++ MINION_SG_NAME=kubernetes-minion-kubernetes
+++ BLOCK_DEVICE_MAPPINGS_BASE='{"DeviceName": "/dev/sdc","VirtualName":"ephemeral0"},{"DeviceName": "/dev/sdd","VirtualName":"ephemeral1"},{"DeviceName": "/dev/sde","VirtualName":"ephemeral2"},{"DeviceName": "/dev/sdf","VirtualName":"ephemeral3"}'
+++ MASTER_BLOCK_DEVICE_MAPPINGS='[{"DeviceName":"/dev/sda1","Ebs":{"DeleteOnTermination":true,"VolumeSize":8,"VolumeType":"gp2"}}, {"DeviceName": "/dev/sdc","VirtualName":"ephemeral0"},{"DeviceName": "/dev/sdd","VirtualName":"ephemeral1"},{"DeviceName": "/dev/sde","VirtualName":"ephemeral2"},{"DeviceName": "/dev/sdf","VirtualName":"ephemeral3"}]'
+++ MINION_BLOCK_DEVICE_MAPPINGS='[{"DeviceName":"/dev/sda1","Ebs":{"DeleteOnTermination":true,"VolumeSize":32,"VolumeType":"gp2"}}, {"DeviceName": "/dev/sdc","VirtualName":"ephemeral0"},{"DeviceName": "/dev/sdd","VirtualName":"ephemeral1"},{"DeviceName": "/dev/sde","VirtualName":"ephemeral2"},{"DeviceName": "/dev/sdf","VirtualName":"ephemeral3"}]'
+ echo '... Starting cluster using provider: aws'
... Starting cluster using provider: aws
+ echo '... calling verify-prereqs'
... calling verify-prereqs
+ verify-prereqs
++ which aws
+ [[ /Users/ilya/Library/Local/Homebrew/bin/aws == '' ]]
+ echo '... calling kube-up'
... calling kube-up
+ kube-up
+ echo 'Starting cluster using os distro: vivid'
Starting cluster using os distro: vivid
+ get-tokens
++ dd if=/dev/urandom bs=128 count=1
++ base64
++ tr -d =+/
++ dd bs=32 count=1
+ KUBELET_TOKEN=s7YeNwAs9PbUlsBx46j6FV5Deql2L5Gg
++ dd if=/dev/urandom bs=128 count=1
++ base64
++ tr -d =+/
++ dd bs=32 count=1
+ KUBE_PROXY_TOKEN=uG2gZPp2n1gX63jeDrWI2huJMTbMkM36
+ detect-image
+ case "${KUBE_OS_DISTRIBUTION}" in
+ detect-vivid-image
+ [[ -z '' ]]
+ case "${AWS_REGION}" in
+ AWS_IMAGE=ami-33566d03
+ detect-minion-image
+ [[ -z '' ]]
+ detect-image
+ case "${KUBE_OS_DISTRIBUTION}" in
+ detect-vivid-image
+ [[ -z ami-33566d03 ]]
+ KUBE_MINION_IMAGE=ami-33566d03
+ find-release-tars
+ SERVER_BINARY_TAR=./../cluster/../cluster/aws/../../cluster/../server/kubernetes-server-linux-amd64.tar.gz
+ [[ ! -f ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-server-linux-amd64.tar.gz ]]
+ [[ ! -f ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-server-linux-amd64.tar.gz ]]
+ SALT_TAR=./../cluster/../cluster/aws/../../cluster/../server/kubernetes-salt.tar.gz
+ [[ ! -f ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-salt.tar.gz ]]
+ [[ ! -f ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-salt.tar.gz ]]
+ ensure-temp-dir
+ [[ -z '' ]]
++ mktemp -d -t kubernetes.XXXXXX
+ KUBE_TEMP=/var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz
+ trap 'rm -rf "${KUBE_TEMP}"' EXIT
+ upload-server-tars
+ SERVER_BINARY_TAR_URL=
+ SALT_TAR_URL=
+ ensure-temp-dir
+ [[ -z /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz ]]
+ [[ -z '' ]]
+ local project_hash=
++ aws configure get aws_access_key_id
+ local key=AKIAJDGHBFIEGW5WH3KQ
+ which md5
++ md5 -q -s 'ilya AKIAJDGHBFIEGW5WH3KQ'
+ project_hash=347034b446ec497d17e00dc92cad862d
+ AWS_S3_BUCKET=kubernetes-staging-347034b446ec497d17e00dc92cad862d
+ echo 'Uploading to Amazon S3'
Uploading to Amazon S3
+ aws s3api get-bucket-location --bucket kubernetes-staging-347034b446ec497d17e00dc92cad862d
+ echo 'Creating kubernetes-staging-347034b446ec497d17e00dc92cad862d'
Creating kubernetes-staging-347034b446ec497d17e00dc92cad862d
+ aws s3 mb s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d --region us-east-1
make_bucket: s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d/
+ local attempt=0
+ true
+ aws s3 ls --region us-east-1 s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d
+ break
++ aws --output text s3api get-bucket-location --bucket kubernetes-staging-347034b446ec497d17e00dc92cad862d
+ local s3_bucket_location=None
+ local s3_url_base=https://s3-None.amazonaws.com
+ [[ None == \N\o\n\e ]]
+ s3_url_base=https://s3.amazonaws.com
+ s3_bucket_location=us-east-1
+ local -r staging_path=devel
+ local -r local_dir=/var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/
+ mkdir /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/
+ echo '+++ Staging server tars to S3 Storage: kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel'
+++ Staging server tars to S3 Storage: kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel
+ local server_binary_path=devel/kubernetes-server-linux-amd64.tar.gz
+ cp -a ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-server-linux-amd64.tar.gz /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/
+ cp -a ./../cluster/../cluster/aws/../../cluster/../server/kubernetes-salt.tar.gz /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/
+ aws s3 sync --region us-east-1 --exact-timestamps /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/ s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/
upload: ../../../../../../var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/kubernetes-salt.tar.gz to s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-salt.tar.gz
upload: ../../../../../../var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/s3/kubernetes-server-linux-amd64.tar.gz to s3://kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-server-linux-amd64.tar.gz
+ aws s3api put-object-acl --region us-east-1 --bucket kubernetes-staging-347034b446ec497d17e00dc92cad862d --key devel/kubernetes-server-linux-amd64.tar.gz --grant-read 'uri="http://acs.amazonaws.com/groups/global/AllUsers"'
+ SERVER_BINARY_TAR_URL=https://s3.amazonaws.com/kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-server-linux-amd64.tar.gz
+ local salt_tar_path=devel/kubernetes-salt.tar.gz
+ aws s3api put-object-acl --region us-east-1 --bucket kubernetes-staging-347034b446ec497d17e00dc92cad862d --key devel/kubernetes-salt.tar.gz --grant-read 'uri="http://acs.amazonaws.com/groups/global/AllUsers"'
+ SALT_TAR_URL=https://s3.amazonaws.com/kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-salt.tar.gz
+ ensure-iam-profiles
+ aws iam get-instance-profile --instance-profile-name kubernetes-master

A client error (NoSuchEntity) occurred when calling the GetInstanceProfile operation: Instance Profile kubernetes-master cannot be found.
+ echo 'Creating master IAM profile: kubernetes-master'
Creating master IAM profile: kubernetes-master
+ create-iam-profile kubernetes-master
+ local key=kubernetes-master
+ local conf_dir=file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam
+ echo 'Creating IAM role: kubernetes-master'
Creating IAM role: kubernetes-master
+ aws iam create-role --role-name kubernetes-master --assume-role-policy-document file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam/kubernetes-master-role.json
+ echo 'Creating IAM role-policy: kubernetes-master'
Creating IAM role-policy: kubernetes-master
+ aws iam put-role-policy --role-name kubernetes-master --policy-name kubernetes-master --policy-document file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam/kubernetes-master-policy.json
+ echo 'Creating IAM instance-policy: kubernetes-master'
Creating IAM instance-policy: kubernetes-master
+ aws iam create-instance-profile --instance-profile-name kubernetes-master
+ echo 'Adding IAM role to instance-policy: kubernetes-master'
Adding IAM role to instance-policy: kubernetes-master
+ aws iam add-role-to-instance-profile --instance-profile-name kubernetes-master --role-name kubernetes-master
+ aws iam get-instance-profile --instance-profile-name kubernetes-minion

A client error (NoSuchEntity) occurred when calling the GetInstanceProfile operation: Instance Profile kubernetes-minion cannot be found.
+ echo 'Creating minion IAM profile: kubernetes-minion'
Creating minion IAM profile: kubernetes-minion
+ create-iam-profile kubernetes-minion
+ local key=kubernetes-minion
+ local conf_dir=file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam
+ echo 'Creating IAM role: kubernetes-minion'
Creating IAM role: kubernetes-minion
+ aws iam create-role --role-name kubernetes-minion --assume-role-policy-document file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam/kubernetes-minion-role.json
+ echo 'Creating IAM role-policy: kubernetes-minion'
Creating IAM role-policy: kubernetes-minion
+ aws iam put-role-policy --role-name kubernetes-minion --policy-name kubernetes-minion --policy-document file://./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/iam/kubernetes-minion-policy.json
+ echo 'Creating IAM instance-policy: kubernetes-minion'
Creating IAM instance-policy: kubernetes-minion
+ aws iam create-instance-profile --instance-profile-name kubernetes-minion
+ echo 'Adding IAM role to instance-policy: kubernetes-minion'
Adding IAM role to instance-policy: kubernetes-minion
+ aws iam add-role-to-instance-profile --instance-profile-name kubernetes-minion --role-name kubernetes-minion
+ load-or-gen-kube-basicauth
+ [[ ! -z '' ]]
+ [[ -z '' ]]
+ gen-kube-basicauth
+ KUBE_USER=admin
++ python -c 'import string,random; print "".join(random.SystemRandom().choice(string.ascii_letters + string.digits) for _ in range(16))'
+ KUBE_PASSWORD=Ne7PaCXPR3FFic70
+ [[ ! -f /Users/ilya/.ssh/kube_aws_rsa ]]
+ ssh-keygen -f /Users/ilya/.ssh/kube_aws_rsa -N ''
Generating public/private rsa key pair.
Your identification has been saved in /Users/ilya/.ssh/kube_aws_rsa.
Your public key has been saved in /Users/ilya/.ssh/kube_aws_rsa.pub.
The key fingerprint is:
SHA256:+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0 ilya@wroom.local
The key's randomart image is:
+---[RSA 2048]----+
|     .=.B+oo .*=.|
|     + E +oo.o  .|
|    . B O   o    |
|     + * O +     |
|      . S X .    |
|       + + =     |
|      . * + .    |
|       o *       |
|        o        |
+----[SHA256]-----+
++ get-ssh-fingerprint /Users/ilya/.ssh/kube_aws_rsa.pub
++ local -r pubkey_path=/Users/ilya/.ssh/kube_aws_rsa.pub
++ ssh-keygen -lf /Users/ilya/.ssh/kube_aws_rsa.pub
++ cut -f2 '-d '
+ AWS_SSH_KEY_FINGERPRINT=SHA256:+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0
+ echo 'Using SSH key with (AWS) fingerprint: SHA256:+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0'
Using SSH key with (AWS) fingerprint: SHA256:+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0
+ AWS_SSH_KEY_NAME=kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0
+ import-public-key kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0 /Users/ilya/.ssh/kube_aws_rsa.pub
+ local -r name=kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0
+ local -r path=/Users/ilya/.ssh/kube_aws_rsa.pub
+ local ok=1
+ local output=
++ aws --output json ec2 import-key-pair --key-name kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0 --public-key-material file:///Users/ilya/.ssh/kube_aws_rsa.pub
+ output='{
    "KeyName": "kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0", 
    "KeyFingerprint": "2c:68:72:fa:c7:64:69:0b:de:1e:29:e1:5a:53:68:8b"
}'
+ [[ 1 == 0 ]]
+ [[ -z '' ]]
++ get_vpc_id
++ aws --output json ec2 --output text describe-vpcs --filters Name=tag:Name,Values=kubernetes-vpc Name=tag:KubernetesCluster,Values=kubernetes --query 'Vpcs[].VpcId'
+ VPC_ID=
+ [[ -z '' ]]
+ echo 'Creating vpc.'
Creating vpc.
++ aws --output json ec2 create-vpc --cidr-block 172.20.0.0/16
++ json_val '["Vpc"]["VpcId"]'
++ python -c 'import json,sys;obj=json.load(sys.stdin);print obj["Vpc"]["VpcId"]'
+ VPC_ID=vpc-5fb9ae3a
+ aws --output json ec2 modify-vpc-attribute --vpc-id vpc-5fb9ae3a --enable-dns-support '{"Value": true}'
+ aws --output json ec2 modify-vpc-attribute --vpc-id vpc-5fb9ae3a --enable-dns-hostnames '{"Value": true}'
+ add-tag vpc-5fb9ae3a Name kubernetes-vpc
+ echo 'Adding tag to vpc-5fb9ae3a: Name=kubernetes-vpc'
Adding tag to vpc-5fb9ae3a: Name=kubernetes-vpc
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources vpc-5fb9ae3a --tags Key=Name,Value=kubernetes-vpc
+ return
+ add-tag vpc-5fb9ae3a KubernetesCluster kubernetes
+ echo 'Adding tag to vpc-5fb9ae3a: KubernetesCluster=kubernetes'
Adding tag to vpc-5fb9ae3a: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources vpc-5fb9ae3a --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ echo 'Using VPC vpc-5fb9ae3a'
Using VPC vpc-5fb9ae3a
+ [[ -z '' ]]
++ aws --output json ec2 describe-subnets --filters Name=tag:KubernetesCluster,Values=kubernetes
++ get_subnet_id vpc-5fb9ae3a us-west-2a
++ python -c 'import json,sys; lst = [str(subnet['\''SubnetId'\'']) for subnet in json.load(sys.stdin)['\''Subnets'\''] if subnet['\''VpcId'\''] == '\''vpc-5fb9ae3a'\'' and subnet['\''AvailabilityZone'\''] == '\''us-west-2a'\'']; print '\'''\''.join(lst)'
+ SUBNET_ID=
+ [[ -z '' ]]
+ echo 'Creating subnet.'
Creating subnet.
++ aws --output json ec2 create-subnet --cidr-block 172.20.0.0/24 --vpc-id vpc-5fb9ae3a --availability-zone us-west-2a
++ json_val '["Subnet"]["SubnetId"]'
++ python -c 'import json,sys;obj=json.load(sys.stdin);print obj["Subnet"]["SubnetId"]'
+ SUBNET_ID=subnet-ed556b9a
+ add-tag subnet-ed556b9a KubernetesCluster kubernetes
+ echo 'Adding tag to subnet-ed556b9a: KubernetesCluster=kubernetes'
Adding tag to subnet-ed556b9a: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources subnet-ed556b9a --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ echo 'Using subnet subnet-ed556b9a'
Using subnet subnet-ed556b9a
++ aws --output json ec2 describe-internet-gateways
++ get_igw_id vpc-5fb9ae3a
++ python -c 'import json,sys; lst = [str(igw['\''InternetGatewayId'\'']) for igw in json.load(sys.stdin)['\''InternetGateways'\''] for attachment in igw['\''Attachments'\''] if attachment['\''VpcId'\''] == '\''vpc-5fb9ae3a'\'']; print '\'''\''.join(lst)'
+ IGW_ID=
+ [[ -z '' ]]
+ echo 'Creating Internet Gateway.'
Creating Internet Gateway.
++ aws --output json ec2 create-internet-gateway
++ json_val '["InternetGateway"]["InternetGatewayId"]'
++ python -c 'import json,sys;obj=json.load(sys.stdin);print obj["InternetGateway"]["InternetGatewayId"]'
+ IGW_ID=igw-aa9cd8cf
+ aws --output json ec2 attach-internet-gateway --internet-gateway-id igw-aa9cd8cf --vpc-id vpc-5fb9ae3a
+ echo 'Using Internet Gateway igw-aa9cd8cf'
Using Internet Gateway igw-aa9cd8cf
+ echo 'Associating route table.'
Associating route table.
++ aws --output json ec2 --output text describe-route-tables --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes --query 'RouteTables[].RouteTableId'
+ ROUTE_TABLE_ID=
+ [[ -z '' ]]
+ echo 'Creating route table'
Creating route table
++ aws --output json ec2 --output text create-route-table --vpc-id=vpc-5fb9ae3a --query RouteTable.RouteTableId
+ ROUTE_TABLE_ID=rtb-7846aa1c
+ add-tag rtb-7846aa1c KubernetesCluster kubernetes
+ echo 'Adding tag to rtb-7846aa1c: KubernetesCluster=kubernetes'
Adding tag to rtb-7846aa1c: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources rtb-7846aa1c --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ echo 'Associating route table rtb-7846aa1c to subnet subnet-ed556b9a'
Associating route table rtb-7846aa1c to subnet subnet-ed556b9a
+ aws --output json ec2 associate-route-table --route-table-id rtb-7846aa1c --subnet-id subnet-ed556b9a
+ echo 'Adding route to route table rtb-7846aa1c'
Adding route to route table rtb-7846aa1c
+ aws --output json ec2 create-route --route-table-id rtb-7846aa1c --destination-cidr-block 0.0.0.0/0 --gateway-id igw-aa9cd8cf
+ echo 'Using Route Table rtb-7846aa1c'
Using Route Table rtb-7846aa1c
++ get_security_group_id kubernetes-master-kubernetes
++ local name=kubernetes-master-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-master-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ MASTER_SG_ID=
+ [[ -z '' ]]
+ echo 'Creating master security group.'
Creating master security group.
+ create-security-group kubernetes-master-kubernetes 'Kubernetes security group applied to master nodes'
+ local -r name=kubernetes-master-kubernetes
+ local -r 'description=Kubernetes security group applied to master nodes'
++ get_security_group_id kubernetes-master-kubernetes
++ local name=kubernetes-master-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-master-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ local sgid=
+ [[ -z '' ]]
+ echo 'Creating security group kubernetes-master-kubernetes.'
Creating security group kubernetes-master-kubernetes.
++ aws --output json ec2 create-security-group --group-name kubernetes-master-kubernetes --description 'Kubernetes security group applied to master nodes' --vpc-id vpc-5fb9ae3a --query GroupId --output text
+ sgid=sg-5f6e9638
+ add-tag sg-5f6e9638 KubernetesCluster kubernetes
+ echo 'Adding tag to sg-5f6e9638: KubernetesCluster=kubernetes'
Adding tag to sg-5f6e9638: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources sg-5f6e9638 --tags Key=KubernetesCluster,Value=kubernetes
+ return
++ get_security_group_id kubernetes-minion-kubernetes
++ local name=kubernetes-minion-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-minion-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ MINION_SG_ID=
+ [[ -z '' ]]
+ echo 'Creating minion security group.'
Creating minion security group.
+ create-security-group kubernetes-minion-kubernetes 'Kubernetes security group applied to minion nodes'
+ local -r name=kubernetes-minion-kubernetes
+ local -r 'description=Kubernetes security group applied to minion nodes'
++ get_security_group_id kubernetes-minion-kubernetes
++ local name=kubernetes-minion-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-minion-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ local sgid=
+ [[ -z '' ]]
+ echo 'Creating security group kubernetes-minion-kubernetes.'
Creating security group kubernetes-minion-kubernetes.
++ aws --output json ec2 create-security-group --group-name kubernetes-minion-kubernetes --description 'Kubernetes security group applied to minion nodes' --vpc-id vpc-5fb9ae3a --query GroupId --output text
+ sgid=sg-5b6e963c
+ add-tag sg-5b6e963c KubernetesCluster kubernetes
+ echo 'Adding tag to sg-5b6e963c: KubernetesCluster=kubernetes'
Adding tag to sg-5b6e963c: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources sg-5b6e963c --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ detect-security-groups
+ [[ -z '' ]]
++ get_security_group_id kubernetes-master-kubernetes
++ local name=kubernetes-master-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-master-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ MASTER_SG_ID=sg-5f6e9638
+ [[ -z sg-5f6e9638 ]]
+ echo 'Using master security group: kubernetes-master-kubernetes sg-5f6e9638'
Using master security group: kubernetes-master-kubernetes sg-5f6e9638
+ [[ -z '' ]]
++ get_security_group_id kubernetes-minion-kubernetes
++ local name=kubernetes-minion-kubernetes
++ tr '\t' '\n'
++ aws --output json ec2 --output text describe-security-groups --filters Name=vpc-id,Values=vpc-5fb9ae3a Name=group-name,Values=kubernetes-minion-kubernetes Name=tag:KubernetesCluster,Values=kubernetes --query 'SecurityGroups[].GroupId'
+ MINION_SG_ID=sg-5b6e963c
+ [[ -z sg-5b6e963c ]]
+ echo 'Using minion security group: kubernetes-minion-kubernetes sg-5b6e963c'
Using minion security group: kubernetes-minion-kubernetes sg-5b6e963c
+ authorize-security-group-ingress sg-5f6e9638 '--source-group sg-5f6e9638 --protocol all'
+ local -r sgid=sg-5f6e9638
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5f6e9638 --source-group sg-5f6e9638 --protocol all
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5b6e963c '--source-group sg-5b6e963c --protocol all'
+ local -r sgid=sg-5b6e963c
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5b6e963c --source-group sg-5b6e963c --protocol all
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5f6e9638 '--source-group sg-5b6e963c --protocol all'
+ local -r sgid=sg-5f6e9638
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5f6e9638 --source-group sg-5b6e963c --protocol all
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5b6e963c '--source-group sg-5f6e9638 --protocol all'
+ local -r sgid=sg-5b6e963c
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5b6e963c --source-group sg-5f6e9638 --protocol all
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5f6e9638 '--protocol tcp --port 22 --cidr 0.0.0.0/0'
+ local -r sgid=sg-5f6e9638
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5f6e9638 --protocol tcp --port 22 --cidr 0.0.0.0/0
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5b6e963c '--protocol tcp --port 22 --cidr 0.0.0.0/0'
+ local -r sgid=sg-5b6e963c
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5b6e963c --protocol tcp --port 22 --cidr 0.0.0.0/0
+ output=
+ [[ 1 == 0 ]]
+ authorize-security-group-ingress sg-5f6e9638 '--protocol tcp --port 443 --cidr 0.0.0.0/0'
+ local -r sgid=sg-5f6e9638
+ shift
+ local ok=1
+ local output=
++ aws --output json ec2 authorize-security-group-ingress --group-id sg-5f6e9638 --protocol tcp --port 443 --cidr 0.0.0.0/0
+ output=
+ [[ 1 == 0 ]]
+ ensure-master-pd
+ local name=kubernetes-master-pd
+ find-master-pd
+ local name=kubernetes-master-pd
+ [[ -z '' ]]
++ aws --output json ec2 --output text describe-volumes --filters Name=availability-zone,Values=us-west-2a Name=tag:Name,Values=kubernetes-master-pd Name=tag:KubernetesCluster,Values=kubernetes --query 'Volumes[].VolumeId'
+ MASTER_DISK_ID=
+ [[ -z '' ]]
+ echo 'Creating master disk: size 20GB, type gp2'
Creating master disk: size 20GB, type gp2
++ aws --output json ec2 create-volume --availability-zone us-west-2a --volume-type gp2 --size 20 --query VolumeId --output text
+ MASTER_DISK_ID=vol-e444c724
+ add-tag vol-e444c724 Name kubernetes-master-pd
+ echo 'Adding tag to vol-e444c724: Name=kubernetes-master-pd'
Adding tag to vol-e444c724: Name=kubernetes-master-pd
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources vol-e444c724 --tags Key=Name,Value=kubernetes-master-pd
+ return
+ add-tag vol-e444c724 KubernetesCluster kubernetes
+ echo 'Adding tag to vol-e444c724: KubernetesCluster=kubernetes'
Adding tag to vol-e444c724: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources vol-e444c724 --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ octets=($(echo "$SERVICE_CLUSTER_IP_RANGE" | sed -e 's|/.*||' -e 's/\./ /g'))
++ echo 10.0.0.0/16
++ sed -e 's|/.*||' -e 's/\./ /g'
+ (( octets[3]+=1 ))
++ echo '10 0 0 1'
++ sed 's/ /./g'
+ service_ip=10.0.0.1
+ MASTER_EXTRA_SANS=IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local,DNS:kubernetes-master
+ echo '#! /bin/bash'
+ echo 'mkdir -p /var/cache/kubernetes-install'
+ echo 'cd /var/cache/kubernetes-install'
+ echo 'readonly SALT_MASTER='\''172.20.0.9'\'''
+ echo 'readonly INSTANCE_PREFIX='\''kubernetes'\'''
+ echo 'readonly NODE_INSTANCE_PREFIX='\''kubernetes-minion'\'''
+ echo 'readonly CLUSTER_IP_RANGE='\''10.244.0.0/16'\'''
+ echo 'readonly ALLOCATE_NODE_CIDRS='\''true'\'''
+ echo 'readonly SERVER_BINARY_TAR_URL='\''https://s3.amazonaws.com/kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-server-linux-amd64.tar.gz'\'''
+ echo 'readonly SALT_TAR_URL='\''https://s3.amazonaws.com/kubernetes-staging-347034b446ec497d17e00dc92cad862d/devel/kubernetes-salt.tar.gz'\'''
+ echo 'readonly ZONE='\''us-west-2a'\'''
+ echo 'readonly KUBE_USER='\''admin'\'''
+ echo 'readonly KUBE_PASSWORD='\''Ne7PaCXPR3FFic70'\'''
+ echo 'readonly SERVICE_CLUSTER_IP_RANGE='\''10.0.0.0/16'\'''
+ echo 'readonly ENABLE_CLUSTER_MONITORING='\''influxdb'\'''
+ echo 'readonly ENABLE_CLUSTER_LOGGING='\''true'\'''
+ echo 'readonly ENABLE_NODE_LOGGING='\''true'\'''
+ echo 'readonly LOGGING_DESTINATION='\''elasticsearch'\'''
+ echo 'readonly ELASTICSEARCH_LOGGING_REPLICAS='\''1'\'''
+ echo 'readonly ENABLE_CLUSTER_DNS='\''true'\'''
+ echo 'readonly ENABLE_CLUSTER_UI='\''true'\'''
+ echo 'readonly DNS_REPLICAS='\''1'\'''
+ echo 'readonly DNS_SERVER_IP='\''10.0.0.10'\'''
+ echo 'readonly DNS_DOMAIN='\''cluster.local'\'''
+ echo 'readonly ADMISSION_CONTROL='\''NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota'\'''
+ echo 'readonly MASTER_IP_RANGE='\''10.246.0.0/24'\'''
+ echo 'readonly KUBELET_TOKEN='\''s7YeNwAs9PbUlsBx46j6FV5Deql2L5Gg'\'''
+ echo 'readonly KUBE_PROXY_TOKEN='\''uG2gZPp2n1gX63jeDrWI2huJMTbMkM36'\'''
+ echo 'readonly DOCKER_STORAGE='\''aufs'\'''
+ echo 'readonly MASTER_EXTRA_SANS='\''IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local,DNS:kubernetes-master'\'''
+ echo 'readonly NUM_MINIONS='\''4'\'''
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/common.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/format-disks.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/setup-master-pd.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/create-dynamic-salt-files.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/download-release.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/salt-master.sh
+ echo 'Starting Master'
Starting Master
++ json_val '["Instances"][0]["InstanceId"]'
++ aws --output json ec2 run-instances --image-id ami-33566d03 --iam-instance-profile Name=kubernetes-master --instance-type t2.micro --subnet-id subnet-ed556b9a --private-ip-address 172.20.0.9 --key-name kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0 --security-group-ids sg-5f6e9638 --associate-public-ip-address --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"DeleteOnTermination":true,"VolumeSize":8,"VolumeType":"gp2"}}, {"DeviceName": "/dev/sdc","VirtualName":"ephemeral0"},{"DeviceName": "/dev/sdd","VirtualName":"ephemeral1"},{"DeviceName": "/dev/sde","VirtualName":"ephemeral2"},{"DeviceName": "/dev/sdf","VirtualName":"ephemeral3"}]' --user-data file:///var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/master-start.sh
++ python -c 'import json,sys;obj=json.load(sys.stdin);print obj["Instances"][0]["InstanceId"]'
+ master_id=i-388868e0
+ add-tag i-388868e0 Name kubernetes-master
+ echo 'Adding tag to i-388868e0: Name=kubernetes-master'
Adding tag to i-388868e0: Name=kubernetes-master
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources i-388868e0 --tags Key=Name,Value=kubernetes-master
+ return
+ add-tag i-388868e0 Role kubernetes-master
+ echo 'Adding tag to i-388868e0: Role=kubernetes-master'
Adding tag to i-388868e0: Role=kubernetes-master
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources i-388868e0 --tags Key=Role,Value=kubernetes-master
+ return
+ add-tag i-388868e0 KubernetesCluster kubernetes
+ echo 'Adding tag to i-388868e0: KubernetesCluster=kubernetes'
Adding tag to i-388868e0: KubernetesCluster=kubernetes
+ n=0
+ '[' 0 -ge 25 ']'
+ aws --output json ec2 create-tags --resources i-388868e0 --tags Key=KubernetesCluster,Value=kubernetes
+ return
+ echo 'Waiting for master to be ready'
Waiting for master to be ready
+ local attempt=0
+ true
+ echo -n Attempt 1 to check for master node
Attempt 1 to check for master node++ get_instance_public_ip i-388868e0
++ local instance_id=i-388868e0
++ aws --output json ec2 --output text describe-instances --instance-ids i-388868e0 --query 'Reservations[].Instances[].NetworkInterfaces[0].Association.PublicIp'
+ local ip=54.213.253.188
+ [[ -z 54.213.253.188 ]]
+ wait-for-instance-running i-388868e0
+ instance_id=i-388868e0
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=i-388868e0
+ [[ i-388868e0 == '' ]]
+ echo 'Waiting for instance i-388868e0 to spawn'
Waiting for instance i-388868e0 to spawn
+ echo 'Sleeping for 3 seconds...'
Sleeping for 3 seconds...
+ sleep 3
+ true
++ aws --output json ec2 describe-instances --instance-ids i-388868e0
++ expect_instance_states running
++ python -c 'import json,sys; lst = [str(instance['\''InstanceId'\'']) for reservation in json.load(sys.stdin)['\''Reservations'\''] for instance in reservation['\''Instances'\''] if instance['\''State'\'']['\''Name'\''] != '\''running'\'']; print '\'' '\''.join(lst)'
+ instance_state=
+ [[ '' == '' ]]
+ break
+ KUBE_MASTER=kubernetes-master
++ assign-elastic-ip 54.213.253.188 i-388868e0
++ local assigned_public_ip=54.213.253.188
++ local master_instance_id=i-388868e0
++ [[ '' =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]
++ [[ '' = \a\u\t\o ]]
++ echo 54.213.253.188
+ KUBE_MASTER_IP=54.213.253.188
+ echo -e ' \033[0;32m[master running @54.213.253.188]\033[0m'
 [master running @54.213.253.188]
+ echo 'Attaching persistent data volume (vol-e444c724) to master'
Attaching persistent data volume (vol-e444c724) to master
+ aws --output json ec2 attach-volume --volume-id vol-e444c724 --device /dev/sdb --instance-id i-388868e0
{
    "AttachTime": "2016-01-29T07:30:18.481Z", 
    "InstanceId": "i-388868e0", 
    "VolumeId": "vol-e444c724", 
    "State": "attaching", 
    "Device": "/dev/sdb"
}
+ sleep 10
+ aws --output json ec2 create-route --route-table-id rtb-7846aa1c --destination-cidr-block 10.246.0.0/24 --instance-id i-388868e0
+ break
+ attempt=0
+ true
+ echo -n Attempt 1 to check for SSH to master
Attempt 1 to check for SSH to master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 uptime
+ output=' 07:31:53 up 1 min,  0 users,  load average: 0.99, 0.25, 0.08'
+ [[ 1 == 0 ]]
+ echo -e ' \033[0;32m[ssh to master working]\033[0m'
 [ssh to master working]
+ break
+ attempt=0
+ true
+ echo -n Attempt 1 to check for salt-master
Attempt 1 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=1
+ sleep 10
+ true
+ echo -n Attempt 2 to check for salt-master
Attempt 2 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=2
+ sleep 10
+ true
+ echo -n Attempt 3 to check for salt-master
Attempt 3 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=3
+ sleep 10
+ true
+ echo -n Attempt 4 to check for salt-master
Attempt 4 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=4
+ sleep 10
+ true
+ echo -n Attempt 5 to check for salt-master
Attempt 5 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=5
+ sleep 10
+ true
+ echo -n Attempt 6 to check for salt-master
Attempt 6 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=6
+ sleep 10
+ true
+ echo -n Attempt 7 to check for salt-master
Attempt 7 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=7
+ sleep 10
+ true
+ echo -n Attempt 8 to check for salt-master
Attempt 8 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=8
+ sleep 10
+ true
+ echo -n Attempt 9 to check for salt-master
Attempt 9 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=9
+ sleep 10
+ true
+ echo -n Attempt 10 to check for salt-master
Attempt 10 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=10
+ sleep 10
+ true
+ echo -n Attempt 11 to check for salt-master
Attempt 11 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=11
+ sleep 10
+ true
+ echo -n Attempt 12 to check for salt-master
Attempt 12 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=12
+ sleep 10
+ true
+ echo -n Attempt 13 to check for salt-master
Attempt 13 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output=
+ ok=0
+ [[ 0 == 0 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m[salt-master not working yet]\033[0m'
 [salt-master not working yet]
+ attempt=13
+ sleep 10
+ true
+ echo -n Attempt 14 to check for salt-master
Attempt 14 to check for salt-master+ local output
+ local ok=1
++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 pgrep salt-master
+ output='14822
14907
14908
14909
14964
14965
14966
14967
14970
14971
14972
14973'
+ [[ 1 == 0 ]]
+ echo -e ' \033[0;32m[salt-master running]\033[0m'
 [salt-master running]
+ break
+ echo 'Creating minion configuration'
Creating minion configuration
+ generate-minion-user-data
+ echo '#! /bin/bash'
+ echo 'SALT_MASTER='\''172.20.0.9'\'''
+ echo 'DOCKER_OPTS='\'''\'''
+ echo 'readonly DOCKER_STORAGE='\''aufs'\'''
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/common.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/format-disks.sh
+ grep -v '^#' ./../cluster/../cluster/aws/../../cluster/../cluster/aws/templates/salt-minion.sh
+ local public_ip_option
+ [[ true == \t\r\u\e ]]
+ public_ip_option=--associate-public-ip-address
+ aws --output json autoscaling create-launch-configuration --launch-configuration-name kubernetes-minion-group --image-id ami-33566d03 --iam-instance-profile kubernetes-minion --instance-type t2.micro --key-name kubernetes-SHA256+sRBIkdUTp1dNYSFMsDOcz5kqwlvSWQPC9ByldooxY0 --security-groups sg-5b6e963c --associate-public-ip-address --block-device-mappings '[{"DeviceName":"/dev/sda1","Ebs":{"DeleteOnTermination":true,"VolumeSize":32,"VolumeType":"gp2"}}, {"DeviceName": "/dev/sdc","VirtualName":"ephemeral0"},{"DeviceName": "/dev/sdd","VirtualName":"ephemeral1"},{"DeviceName": "/dev/sde","VirtualName":"ephemeral2"},{"DeviceName": "/dev/sdf","VirtualName":"ephemeral3"}]' --user-data file:///var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz/minion-user-data
+ echo 'Creating autoscaling group'
Creating autoscaling group
+ aws --output json autoscaling create-auto-scaling-group --auto-scaling-group-name kubernetes-minion-group --launch-configuration-name kubernetes-minion-group --min-size 4 --max-size 4 --vpc-zone-identifier subnet-ed556b9a --tags ResourceId=kubernetes-minion-group,ResourceType=auto-scaling-group,Key=Name,Value=kubernetes-minion ResourceId=kubernetes-minion-group,ResourceType=auto-scaling-group,Key=Role,Value=kubernetes-minion ResourceId=kubernetes-minion-group,ResourceType=auto-scaling-group,Key=KubernetesCluster,Value=kubernetes
+ attempt=0
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ [[ 0 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m0 minions started; waiting\033[0m'
 0 minions started; waiting
+ attempt=1
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ [[ 0 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m0 minions started; waiting\033[0m'
 0 minions started; waiting
+ attempt=2
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ [[ 0 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m0 minions started; waiting\033[0m'
 0 minions started; waiting
+ attempt=3
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ [[ 0 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m0 minions started; waiting\033[0m'
 0 minions started; waiting
+ attempt=4
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ [[ 1 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m1 minions started; waiting\033[0m'
 1 minions started; waiting
+ attempt=5
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ [[ 1 == 4 ]]
+ ((  attempt > 30  ))
+ echo -e ' \033[0;33m1 minions started; waiting\033[0m'
 1 minions started; waiting
+ attempt=6
+ sleep 10
+ true
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ [[ 4 == 4 ]]
+ echo -e ' \033[0;32m4 minions started; ready\033[0m'
 4 minions started; ready
+ break
+ detect-master
+ KUBE_MASTER=kubernetes-master
+ [[ -z '' ]]
++ get_instanceid_from_name kubernetes-master
++ local tagName=kubernetes-master
++ aws --output json ec2 --output text describe-instances --filters Name=tag:Name,Values=kubernetes-master Name=instance-state-name,Values=running Name=tag:KubernetesCluster,Values=kubernetes --query 'Reservations[].Instances[].InstanceId'
+ KUBE_MASTER_ID=i-388868e0
+ [[ -z i-388868e0 ]]
+ [[ -z 54.213.253.188 ]]
+ [[ -z 54.213.253.188 ]]
+ echo 'Using master: kubernetes-master (external IP: 54.213.253.188)'
+ detect-minions
+ find-running-minions
+ MINION_IDS=()
+ MINION_NAMES=()
++ query-running-minions 'Reservations[].Instances[].InstanceId'
++ local 'query=Reservations[].Instances[].InstanceId'
++ aws --output json ec2 --output text describe-instances --filters Name=instance-state-name,Values=running Name=vpc-id,Values=vpc-5fb9ae3a Name=tag:KubernetesCluster,Values=kubernetes Name=tag:Role,Values=kubernetes-minion --query 'Reservations[].Instances[].InstanceId'
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ for id in '$(query-running-minions "Reservations[].Instances[].InstanceId")'
+ MINION_IDS+=("${id}")
+ MINION_NAMES+=("${id}")
+ KUBE_MINION_IP_ADDRESSES=()
+ (( i=0 ))
+ (( i<4 ))
+ local minion_ip
+ [[ true == \t\r\u\e ]]
++ get_instance_public_ip i-328b6bea
++ local instance_id=i-328b6bea
++ aws --output json ec2 --output text describe-instances --instance-ids i-328b6bea --query 'Reservations[].Instances[].NetworkInterfaces[0].Association.PublicIp'
+ minion_ip=54.191.163.221
+ echo 'Found minion 0: i-328b6bea @ 54.191.163.221'
+ KUBE_MINION_IP_ADDRESSES+=("${minion_ip}")
+ (( i++ ))
+ (( i<4 ))
+ local minion_ip
+ [[ true == \t\r\u\e ]]
++ get_instance_public_ip i-348b6bec
++ local instance_id=i-348b6bec
++ aws --output json ec2 --output text describe-instances --instance-ids i-348b6bec --query 'Reservations[].Instances[].NetworkInterfaces[0].Association.PublicIp'
+ minion_ip=54.200.73.193
+ echo 'Found minion 1: i-348b6bec @ 54.200.73.193'
+ KUBE_MINION_IP_ADDRESSES+=("${minion_ip}")
+ (( i++ ))
+ (( i<4 ))
+ local minion_ip
+ [[ true == \t\r\u\e ]]
++ get_instance_public_ip i-358b6bed
++ local instance_id=i-358b6bed
++ aws --output json ec2 --output text describe-instances --instance-ids i-358b6bed --query 'Reservations[].Instances[].NetworkInterfaces[0].Association.PublicIp'
+ minion_ip=54.200.138.242
+ echo 'Found minion 2: i-358b6bed @ 54.200.138.242'
+ KUBE_MINION_IP_ADDRESSES+=("${minion_ip}")
+ (( i++ ))
+ (( i<4 ))
+ local minion_ip
+ [[ true == \t\r\u\e ]]
++ get_instance_public_ip i-378b6bef
++ local instance_id=i-378b6bef
++ aws --output json ec2 --output text describe-instances --instance-ids i-378b6bef --query 'Reservations[].Instances[].NetworkInterfaces[0].Association.PublicIp'
+ minion_ip=54.191.203.75
+ echo 'Found minion 3: i-378b6bef @ 54.191.203.75'
+ KUBE_MINION_IP_ADDRESSES+=("${minion_ip}")
+ (( i++ ))
+ (( i<4 ))
+ [[ -z 54.191.163.221 ]]
+ echo 'Waiting 3 minutes for cluster to settle'
Waiting 3 minutes for cluster to settle
+ local i
+ (( i=0 ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ printf .
.+ sleep 10
+ (( i++ ))
+ (( i < 6*3 ))
+ echo 'Re-running salt highstate'
Re-running salt highstate
+ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 sudo salt '*' state.highstate
+ echo 'Waiting for cluster initialization.'
Waiting for cluster initialization.
+ echo

+ echo '  This will continually check to see if the API for kubernetes is reachable.'
  This will continually check to see if the API for kubernetes is reachable.
+ echo '  This might loop forever if there was some uncaught error during start'
  This might loop forever if there was some uncaught error during start
+ echo '  up.'
  up.
+ echo

++ curl --insecure --user admin:Ne7PaCXPR3FFic70 --max-time 5 --fail --output /dev/null --silent https://54.213.253.188/healthz
+ echo 'Kubernetes cluster created.'
Kubernetes cluster created.
+ export KUBE_CERT=/tmp/19598-kubecfg.crt
+ KUBE_CERT=/tmp/19598-kubecfg.crt
+ export KUBE_KEY=/tmp/8649-kubecfg.key
+ KUBE_KEY=/tmp/8649-kubecfg.key
+ export CA_CERT=/tmp/28161-kubernetes.ca.crt
+ CA_CERT=/tmp/28161-kubernetes.ca.crt
+ export CONTEXT=aws_kubernetes
+ CONTEXT=aws_kubernetes
+ local kubectl=./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh
+ umask 077
+ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 sudo cat /srv/kubernetes/kubecfg.crt
+ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 sudo cat /srv/kubernetes/kubecfg.key
+ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.213.253.188 sudo cat /srv/kubernetes/ca.crt
+ create-kubeconfig
+ local kubectl=./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh
+ export KUBECONFIG=/Users/ilya/.kube/config
+ KUBECONFIG=/Users/ilya/.kube/config
+ [[ ! -e /Users/ilya/.kube/config ]]
+ cluster_args=("--server=${KUBE_SERVER:-https://${KUBE_MASTER_IP}}")
+ local cluster_args
+ [[ -z /tmp/28161-kubernetes.ca.crt ]]
+ cluster_args+=("--certificate-authority=${CA_CERT}" "--embed-certs=true")
+ user_args=()
+ local user_args
+ [[ ! -z '' ]]
+ [[ ! -z admin ]]
+ [[ ! -z Ne7PaCXPR3FFic70 ]]
+ user_args+=("--username=${KUBE_USER}" "--password=${KUBE_PASSWORD}")
+ [[ ! -z /tmp/19598-kubecfg.crt ]]
+ [[ ! -z /tmp/8649-kubecfg.key ]]
+ user_args+=("--client-certificate=${KUBE_CERT}" "--client-key=${KUBE_KEY}" "--embed-certs=true")
+ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config set-cluster aws_kubernetes --server=https://54.213.253.188 --certificate-authority=/tmp/28161-kubernetes.ca.crt --embed-certs=true
cluster "aws_kubernetes" set.
+ [[ -n --username=admin --password=Ne7PaCXPR3FFic70 --client-certificate=/tmp/19598-kubecfg.crt --client-key=/tmp/8649-kubecfg.key --embed-certs=true ]]
+ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config set-credentials aws_kubernetes --username=admin --password=Ne7PaCXPR3FFic70 --client-certificate=/tmp/19598-kubecfg.crt --client-key=/tmp/8649-kubecfg.key --embed-certs=true
user "aws_kubernetes" set.
+ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config set-context aws_kubernetes --cluster=aws_kubernetes --user=aws_kubernetes
context "aws_kubernetes" set.
+ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config use-context aws_kubernetes --cluster=aws_kubernetes
switched to context "aws_kubernetes".
+ [[ ! -z '' ]]
+ echo 'Wrote config for aws_kubernetes to /Users/ilya/.kube/config'
Wrote config for aws_kubernetes to /Users/ilya/.kube/config
+ echo 'Sanity checking cluster...'
Sanity checking cluster...
+ sleep 5
+ set +e
+ local rc
+ (( i=0 ))
+ (( i<4 ))
+ local attempt=0
+ true
+ local minion_ip=54.191.163.221
+ echo -n 'Attempt 1 to check Docker on node @ 54.191.163.221 ...'
Attempt 1 to check Docker on node @ 54.191.163.221 ...++ check-minion 54.191.163.221
++ local minion_ip=54.191.163.221
+++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.191.163.221 sudo docker ps -a
++ local 'output=CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS              PORTS               NAMES
d68dee5120a2        gcr.io/google_containers/pause:0.8.0   "/pause"            17 seconds ago      Up 17 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-35.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_a2d8be91   '
++ [[ -z CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS              PORTS               NAMES
d68dee5120a2        gcr.io/google_containers/pause:0.8.0   "/pause"            17 seconds ago      Up 17 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-35.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_a2d8be91    ]]
++ echo working
+ local output=working
+ echo working
working
+ [[ working != \w\o\r\k\i\n\g ]]
+ break
+ (( i++ ))
+ (( i<4 ))
+ local attempt=0
+ true
+ local minion_ip=54.200.73.193
+ echo -n 'Attempt 1 to check Docker on node @ 54.200.73.193 ...'
Attempt 1 to check Docker on node @ 54.200.73.193 ...++ check-minion 54.200.73.193
++ local minion_ip=54.200.73.193
+++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.200.73.193 sudo docker ps -a
++ local 'output=CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS              PORTS               NAMES
4c8894b6626a        gcr.io/google_containers/pause:0.8.0   "/pause"            19 seconds ago      Up 18 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-33.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_4312cc39   '
++ [[ -z CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS              PORTS               NAMES
4c8894b6626a        gcr.io/google_containers/pause:0.8.0   "/pause"            19 seconds ago      Up 18 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-33.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_4312cc39    ]]
++ echo working
+ local output=working
+ echo working
working
+ [[ working != \w\o\r\k\i\n\g ]]
+ break
+ (( i++ ))
+ (( i<4 ))
+ local attempt=0
+ true
+ local minion_ip=54.200.138.242
+ echo -n 'Attempt 1 to check Docker on node @ 54.200.138.242 ...'
Attempt 1 to check Docker on node @ 54.200.138.242 ...++ check-minion 54.200.138.242
++ local minion_ip=54.200.138.242
+++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.200.138.242 sudo docker ps -a
++ local 'output=CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS                  PORTS               NAMES
1a1eab78548e        gcr.io/google_containers/pause:0.8.0   "/pause"            1 seconds ago       Up Less than a second                       k8s_POD.b0930213_kibana-logging-v1-4pso1_kube-system_ec8f8d06-c65a-11e5-84e4-06f8f1841213_6fa0ccdd                                       
cc5a24435bd0        gcr.io/google_containers/pause:0.8.0   "/pause"            1 seconds ago       Up Less than a second                       k8s_POD.63f848fb_kube-dns-v9-hqxxl_kube-system_ec9ec135-c65a-11e5-84e4-06f8f1841213_fea94151                                             
5e4d18c58974        gcr.io/google_containers/pause:0.8.0   "/pause"            32 seconds ago      Up 31 seconds                               k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-34.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_784d1973   '
++ [[ -z CONTAINER ID        IMAGE                                  COMMAND             CREATED             STATUS                  PORTS               NAMES
1a1eab78548e        gcr.io/google_containers/pause:0.8.0   "/pause"            1 seconds ago       Up Less than a second                       k8s_POD.b0930213_kibana-logging-v1-4pso1_kube-system_ec8f8d06-c65a-11e5-84e4-06f8f1841213_6fa0ccdd                                       
cc5a24435bd0        gcr.io/google_containers/pause:0.8.0   "/pause"            1 seconds ago       Up Less than a second                       k8s_POD.63f848fb_kube-dns-v9-hqxxl_kube-system_ec9ec135-c65a-11e5-84e4-06f8f1841213_fea94151                                             
5e4d18c58974        gcr.io/google_containers/pause:0.8.0   "/pause"            32 seconds ago      Up 31 seconds                               k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-34.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_784d1973    ]]
++ echo working
+ local output=working
+ echo working
working
+ [[ working != \w\o\r\k\i\n\g ]]
+ break
+ (( i++ ))
+ (( i<4 ))
+ local attempt=0
+ true
+ local minion_ip=54.191.203.75
+ echo -n 'Attempt 1 to check Docker on node @ 54.191.203.75 ...'
Attempt 1 to check Docker on node @ 54.191.203.75 ...++ check-minion 54.191.203.75
++ local minion_ip=54.191.203.75
+++ ssh -oStrictHostKeyChecking=no -i /Users/ilya/.ssh/kube_aws_rsa ubuntu@54.191.203.75 sudo docker ps -a
++ local 'output=CONTAINER ID        IMAGE                                                 COMMAND             CREATED             STATUS              PORTS               NAMES
4d3aae24aab7        gcr.io/google_containers/fluentd-elasticsearch:1.11   "td-agent -q"       3 seconds ago       Up 3 seconds                            k8s_fluentd-elasticsearch.6fd5928c_fluentd-elasticsearch-ip-172-20-0-36.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_5dcb14ed   
b636158ea273        gcr.io/google_containers/pause:0.8.0                  "/pause"            7 seconds ago       Up 7 seconds                            k8s_POD.6d00e006_heapster-v10-mraei_kube-system_ec93312b-c65a-11e5-84e4-06f8f1841213_95da1c21                                                              
d079a2eaa924        gcr.io/google_containers/pause:0.8.0                  "/pause"            39 seconds ago      Up 39 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-36.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_9f2e3af0                     '
++ [[ -z CONTAINER ID        IMAGE                                                 COMMAND             CREATED             STATUS              PORTS               NAMES
4d3aae24aab7        gcr.io/google_containers/fluentd-elasticsearch:1.11   "td-agent -q"       3 seconds ago       Up 3 seconds                            k8s_fluentd-elasticsearch.6fd5928c_fluentd-elasticsearch-ip-172-20-0-36.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_5dcb14ed   
b636158ea273        gcr.io/google_containers/pause:0.8.0                  "/pause"            7 seconds ago       Up 7 seconds                            k8s_POD.6d00e006_heapster-v10-mraei_kube-system_ec93312b-c65a-11e5-84e4-06f8f1841213_95da1c21                                                              
d079a2eaa924        gcr.io/google_containers/pause:0.8.0                  "/pause"            39 seconds ago      Up 39 seconds                           k8s_POD.6d00e006_fluentd-elasticsearch-ip-172-20-0-36.us-west-2.compute.internal_kube-system_6c5833c13a26d9feec183f5c07e2616c_9f2e3af0                      ]]
++ echo working
+ local output=working
+ echo working
working
+ [[ working != \w\o\r\k\i\n\g ]]
+ break
+ (( i++ ))
+ (( i<4 ))
+ get-kubeconfig-basicauth
+ export KUBECONFIG=/Users/ilya/.kube/config
+ KUBECONFIG=/Users/ilya/.kube/config
++ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config view -o 'jsonpath={.current-context}'
+ local cc=aws_kubernetes
+ [[ ! -z '' ]]
++ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config view -o 'jsonpath={.contexts[?(@.name == "aws_kubernetes")].context.user}'
+ local user=aws_kubernetes
++ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config view -o 'jsonpath={.users[?(@.name == "aws_kubernetes")].user.username}'
+ KUBE_USER=admin
++ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh config view -o 'jsonpath={.users[?(@.name == "aws_kubernetes")].user.password}'
+ KUBE_PASSWORD=Ne7PaCXPR3FFic70
+ echo

+ echo -e '\033[0;32mKubernetes cluster is running.  The master is running at:'
Kubernetes cluster is running.  The master is running at:
+ echo

+ echo -e '\033[0;33m  https://54.213.253.188'
  https://54.213.253.188
+ echo

+ echo -e '\033[0;32mThe user name and password to use is located in /Users/ilya/.kube/config.\033[0m'
The user name and password to use is located in /Users/ilya/.kube/config.
+ echo

+ echo '... calling validate-cluster'
... calling validate-cluster
+ validate-cluster
+ ./../cluster/../cluster/aws/../../cluster/../cluster/validate-cluster.sh
Found 4 node(s).
NAME                                        LABELS                                                             STATUS    AGE
ip-172-20-0-33.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-33.us-west-2.compute.internal   Ready     1m
ip-172-20-0-34.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-34.us-west-2.compute.internal   Ready     1m
ip-172-20-0-35.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-35.us-west-2.compute.internal   Ready     1m
ip-172-20-0-36.us-west-2.compute.internal   kubernetes.io/hostname=ip-172-20-0-36.us-west-2.compute.internal   Ready     1m
Validate output:
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   nil
scheduler            Healthy   ok                   nil
etcd-0               Healthy   {"health": "true"}   nil
etcd-1               Healthy   {"health": "true"}   nil
Cluster validation succeeded
+ echo -e 'Done, listing cluster services:\n'
Done, listing cluster services:

+ ./../cluster/../cluster/aws/../../cluster/../cluster/kubectl.sh cluster-info
Kubernetes master is running at https://54.213.253.188
Elasticsearch is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/elasticsearch-logging
Heapster is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/heapster
Kibana is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/kibana-logging
KubeDNS is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/kube-dns
KubeUI is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/kube-ui
Grafana is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
InfluxDB is running at https://54.213.253.188/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
+ echo

+ exit 0
+ rm -rf /var/folders/zr/qjxvg8xn10nd06j2vflwtj880000gn/T/kubernetes.XXXXXX.At6tcAqz

0 %> 
